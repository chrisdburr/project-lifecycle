{"config":{"indexing":"full","jieba_dict":null,"jieba_dict_user":null,"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])"},"docs":[{"location":"","text":"<p>'Anatomy of an AI system' is a visual schematic of an AI system by Kate Crawford and Vladan Joler, which captures the far-reaching and expansive effects of artificial intelligence (AI) development. Although they use an Amazon Echo device as their illustrative example, the graphic renders and makes manifest the scale of AI technologies more generally. Distribution, assembly, and manufacturing chains; domestic and information infrastructure; geological processes; content providers and internet platforms; and skilled and unskilled human labour. All of these domains, and many more, are linked together and subsumed by Crawford and Joler's impressive model.</p> <p>In a singular image, the expansive reach of an AI project is laid bare, exposing the myriad sectors and domains that are affected by the pursuit of technological innovation, and the interdependence of artificial intelligence on existing sociocultural practices and institutions.</p>  <p>However, as the oft-quoted phrase, \"all models are wrong, but some are useful\", emphasises, there are limits to what any individual model can achieve. And, like most models, 'Anatomy of an AI System' is designed to support a specific goal or set of objectives, such as representing a physical or sociological process to enable greater theoretical understanding. If 'Anatomy of an AI System' were to be used as a model to help regulators and policy-makers identify and evaluate the most salient activities of a typical AI lifecycle, it would likely cause many to miss the forest for the trees. Therefore, a more tailored model is necessary to support regulators and policy-makers.</p> <p>Such a model should be able to strike a delicate (or, fragile) balance between, on the one hand, the high-level perspective necessary for regulating technology chains with global reach, and on the other hand, the lower-level and detail-orientated perspective required to understand complex, data-driven technologies. The model we present in this guidance achieves this balance, while also serving several additional and vital policy objectives.</p> <p>From the outset, our model of the AI project lifecycle was designed to meet the following, interlocking objectives:</p> <ol> <li>Provide a clear and accessible representation of a typical AI project lifecycle, which has both a high-level of technical accuracy but is also flexible across different domains and organisations (e.g., applicable to both SMEs and multi-national corporations).</li> <li>Support the identification of a) salient regulatory touchpoints, b) value chains and routes to market, and c) legal considerations.</li> <li>Facilitate ongoing discussion into and assessment of legislative and regulatory gaps.</li> <li>Build on existing guidance and best practices in a consistent and complementary manner.</li> <li>Promotes a more responsible and ethical approach to the design, development, and deployment of AI systems, by embedding a bias-aware and human rights focused methodology into the project lifecycle.</li> </ol> <p>The following pages introduces the AI project lifecycle and the constituent stages and activities before turning to a discussion and exploration of how this model facilitates the identification of salient regulatory touchpoints.</p>","title":"Introduction"},{"location":"assets/data/case-studies/","text":"","title":"The SAFE-D Principles: Operationalising Responsible Research and Innovation"},{"location":"assets/data/case-studies/#case-studies","text":"","title":"Case Studies"},{"location":"assets/data/case-studies/#title","text":"<ul> <li>OECD Category: Recognition</li> </ul>","title":"Title"},{"location":"assets/data/case-studies/#title_1","text":"<ul> <li>OECD Category: Event Detection</li> </ul>","title":"Title"},{"location":"assets/data/case-studies/#title_2","text":"","title":"Title"},{"location":"assets/data/case-studies/#-oecd-category-forecasting","text":"","title":"- OECD Category: Forecasting"},{"location":"assets/data/case-studies/#title_3","text":"<ul> <li>OECD Category: Personalisation</li> </ul>","title":"Title"},{"location":"assets/data/case-studies/#title_4","text":"<ul> <li>OECD Category: Interaction Support</li> </ul>","title":"Title"},{"location":"assets/data/case-studies/#title_5","text":"<ul> <li>OECD Category: Goal Driven Optimisation</li> </ul>","title":"Title"},{"location":"assets/data/case-studies/#title_6","text":"<ul> <li>OECD Category: Reasoning with Knowledge Structures</li> </ul>","title":"Title"},{"location":"assurance/as-introduction/","text":"<p></p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus bibendum congue consectetur. Vivamus varius sollicitudin tincidunt. Fusce rutrum egestas orci quis lacinia. Fusce id leo eget elit gravida fermentum ac nec neque. Quisque ac tincidunt ligula. Ut eget lorem eros. Nunc id vestibulum arcu. Suspendisse maximus quam quis pellentesque ullamcorper. Integer volutpat lacus in eros placerat, ut elementum purus venenatis. Phasellus mattis tincidunt enim, ac commodo sem auctor convallis. Quisque at felis nulla. Fusce hendrerit vehicula erat, vitae laoreet odio. Aenean sodales nunc et enim suscipit egestas. Pellentesque elementum, eros eget posuere efficitur, quam ex congue tortor, at egestas risus odio sit amet turpis. Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p> <p>Phasellus ante arcu, tempor sed lacus sit amet, cursus interdum urna. Proin vel ultricies nibh. Vestibulum ex mauris, ornare eu porttitor ut, interdum non mauris. Vivamus mattis ut odio ac fermentum. Integer non congue mauris. Nunc porttitor posuere sapien in efficitur. Fusce ornare fermentum turpis id gravida. Nam at felis a dui tristique efficitur nec consectetur justo.</p> <p>Ut cursus purus vel urna euismod, non egestas nulla consectetur. Mauris scelerisque turpis massa, et sodales dui aliquam et. In eget dolor convallis, sollicitudin neque posuere, tristique velit. Nunc id volutpat velit. Suspendisse consectetur libero ut lacus venenatis facilisis. Nullam vitae nunc sollicitudin erat tempor fringilla et ut ligula. Suspendisse at sem libero. Donec id orci id lorem ultricies accumsan a in elit. Donec a dolor enim. Sed non luctus purus. Aliquam convallis blandit augue in pretium. Maecenas id quam lacus. Quisque turpis lorem, consequat vitae dignissim nec, porttitor et nisi. In condimentum ante vitae nisl pellentesque gravida. Nam at magna vitae metus tristique aliquam.</p>","title":"Trustworthy AI and Assurance"},{"location":"assurance/as-introduction/#overview","text":"<ul> <li> <p> Section 1</p>  <p>Section summary...</p> <p> Go to section</p> </li> <li> <p> Section 2</p>  <p>Section summary...</p> <p> Go to section</p> </li> <li> <p> Section 3</p>  <p>Section summary...</p> <p> Go to section</p> </li> </ul>","title":"Overview"},{"location":"assurance/safe-d-principles/","text":"<p>The SAFE-D principles are normative principles that provide practical guardrails throughout the design, development, and deployment of data-driven technologies, such as ML models or AI systems. The five principles are as follows:</p> <ul> <li>Sustainability</li> <li>Accountability</li> <li>Fairness</li> <li>Explainability</li> <li>Data (Quality, Integrity, and Protection and Privacy)</li> </ul> <p>The 'Data' principle is different from the first four, insofar as it is only indirectly linked to key ethical requirements\u2014hence the separation from the other principles. Nevertheless, its legal and regulatory scope is fundamentally connected to normative values, such as a) considerations of human rights (e.g., respect for privacy), b) methods of responsible data governance (e.g., reproducibility of methods), and c) core democratic values (e.g., decisional autonomy). Therefore, it is appropriate to include alongside the more explicitly ethical principles.</p> <p>Each principle serves as a starting point for a project team's reflective and deliberative activities. Moreover, the principles also serve to enable a common evaluative framework, which can be used by both developers and regulators when communicating regulatory requirements (e.g., legal compliance, auditing, or co-development of best practices).</p> <p>Each principle comprises a series of core attributes, which are designed to further specify the practice-based demands of the principles and also help identify specific actions or decisions that are likely to be required in a specific policy domain (e.g., healthcare, education, employment).</p> <p>This page introduces and defines the five principles and their core attributes. Following this introduction, the following page offers examples of actions that a project team could take at a specific stage in the project lifecycle to provide assurance that they have considered the regulatory and normative requirements of the principle.</p>","title":"The SAFE-D Principles: Operationalising Responsible Research and Innovation"},{"location":"assurance/safe-d-principles/#sustainability","text":"<p>Definition</p> <p>Sustainability can mean a couple of things. From a technical perspective, sustainability requires the outputs of a project to be safe, secure, robust, and reliable. For example, if an organisation is developing an autonomous vehicle, it should operate safely in the intended context of use. However, in the context of responsible data science and AI, there is also a social sustainability component. This aspect of sustainability requires a project\u2019s practices to be informed by ongoing consideration of the risk of exposing individuals to harms even after the system has been deployed and the project completed\u2014a long-term (or sustainable) safety.</p>     Core Attribute Description     Safety Safety is core to sustainability but goes beyond the mere operational safety of the system. It also includes an understanding of the long-term use context and impact of the system, and the resources needed to ensure the system continues to operate safely over time within its environment (i.e. is sustainable). For instance, safety may depend upon sufficient change monitoring processes that establish whether there has been any substantive drift in the underlying data distributions or social operating environment. Or, it could also involve engaging and involving users and stakeholders in the design and assessment of AI systems that could impact their human rights and fundamental freedoms.   Security Security encompasses the protection of several operational dimensions of an AI system when confronted with possible adversarial attack. A secure system is capable of maintaining the integrity of its constitutive information. This includes protecting its architecture from the unauthorised modification or damage of any of its component parts. A secure system also remains continuously functional and accessible to its authorised users and keeps confidential and private information secure even under hostile or adversarial conditions.   Robustness The objective of robustness can be thought of as the goal that an AI system functions reliably and accurately under harsh or uncertain conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is, therefore, the strength of a system\u2019s functional integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, or undesirable reinforcement learning behaviour.   Reliability The objective of reliability is that an AI system behaves exactly as its designers intended and anticipated. A reliable system adheres to the specifications it was programmed to carry out. Reliability is therefore a measure of consistency and can establish confidence in the safety of a system based upon the dependability with which it conforms to its intended functionality.   Accuracy and Performance The accuracy of a model is the proportion of examples for which it generates a correct output. This performance measure is also sometimes characterised conversely as an error rate or the fraction of cases for which the model produces an incorrect output. Specifying a reasonable performance level for the system may also require refining or exchanging the measure of accuracy. For instance, if certain errors are more significant or costly than others, a metric for total cost can be integrated into the model so that the cost of one class of errors can be weighed against that of another.","title":"Sustainability"},{"location":"assurance/safe-d-principles/#accountability","text":"<p>Definition</p> <p>Accountability can refer to transparency of processes and associated outcomes that enable people to understand how a project was conducted (e.g., project documentation), or why a specific decision was reached. But it can also refer to broader processes of responsible project governance that seek to establish clear roles of responsibility where full transparency may be inappropriate (e.g., confidential projects).</p>     Core Attribute Description     Traceability Traceability refers to the process by which all stages of the data lifecycle from collection to deployment to system updating or deprovisioning are documented in a way that is accessible and easily understood. This may include not only the parties within the organisation involved but also the actions taken at each stage that may impact the individuals who use the system.   Answerability Answerability depends upon a human chain of responsibility. Answerability responds to the question of who is accountable for an automation supported outcome.   Auditability Whereas the property of answerability responds to the question of who is accountable for an automation supported outcome, the notion of auditability answers the question of how the designers and implementers of AI systems are to be held accountable. This aspect of accountability has to do with demonstrating and evidencing both the responsibility of design and use practices and the justifiability of outcomes.   Clear Data Provenance and Lineage Clear provenance and data lineage consists of records that are accessible and simultaneously detail how data was collected and how it has been used and altered throughout the stages of pre-processing, modelling, training, testing, and deploying.   Accessibility Accessibility involves ensuring that information about the processes that took place to design, develop, and deploy an AI system are easily accessible by individuals. This not only refers to suitable means of explanation (clear, understandable, and accessible language) but also the mediums for delivery.   Reproducibility Related to and dependant on the above four properties, reproducibility refers to the ability for others to reproduce the steps you have taken throughout your project to achieve the desired outcomes and where necessary to replicate the same outcomes by following the same procedure.","title":"Accountability"},{"location":"assurance/safe-d-principles/#fairness","text":"<p>Definition</p> <p>Fairness is inseparably connected with legal conceptions of equality and justice, which may emphasize a variety of features such as non-discrimination, equitable outcomes, or procedural fairness through bias mitigation. However, these notions serve as a subset of broader normative considerations pertaining to social justice, socioeconomic capabilities, diversity and inclusivity. For this reason, the term \u2018fairness\u2019 can be confusing due to the wide variety of ways it is employed, and the large number of more specific concepts that fall within its scope.</p>     Core Attribute Description     Bias Mitigation It is not possible to eliminate bias entirely. However, effective bias mitigation processes can minimise the unwanted and undesirable impact of systematic deviations, distortions, or disparate outcomes that arise to a project governance problem, interfering factor, or from insufficient reflection on historical social or structural discrimination.   Diversity and Inclusiveness A significant component of fairness aware design is ensuring the inclusion of diverse voices and opinions in the design and development process through the participation of a more representative range of stakeholders. This includes considering whether values of civic participation, inclusion, and diversity been adequately considered in articulating the purpose and setting the goals of the project. Consulting with internal organisational stakeholders is also necessary to strengthen the openness, inclusiveness, and diversity of the project.   Non-Discrimination A system or model should not create or contribute to circumstances whereby members of protected groups are treated differently or less favourably than other groups because of their respective protected characteristic.   Equality the outcome or impact of a system should either maintain or promote a state of affairs in which every individual has equal rights and liberties, and equal access or opportunities to whatever good or service the AI system brings about.","title":"Fairness"},{"location":"assurance/safe-d-principles/#explainability","text":"<p>Definition</p> <p>Explainability is a key condition for autonomous and informed decision-making in situations where data-driven systems interact with or influence human judgement and choice behaviour. Explainability goes beyond the ability to merely interpret specific aspects of a project (e.g., interpreting the parameters of a model); it also depends on the ability to provide an accessible and relevant information base about the processes behind the outcome.</p>     Core Attribute Description     Interpretability Interpretability consists of the ability to know how and why a model performed the way it did in a specific context and, therefore, to understand the rationale behind its decision or behaviour.   Responsible Model Selection The normal expectations of intelligibility and accessibility that accompany the function of the system, as fulfilled in the sector or domain in which it will operate. This can also necessitate the availability of more interpretable algorithmic models or techniques in cases where the selection of an opaque model poses risks to the physical, psychological, or moral integrity of rights-holders or to their human rights and fundamental freedoms. The availability of the resources and capacity that will be needed to provide responsible, supplementary methods of explanation (e.g. simpler surrogate models, sensitivity analysis, or relative feature important) in cases where an opaque model is deemed appropriate and selected.   Accessible Rationale Explanation The reasons that led to a decision\u2014especially one that is automated\u2014delivered in an accessible and non-technical way.   Implementation and User Training Training users to operate the AI system may include: a) conveying basic knowledge about the nature of machine learning, b) explaining the limitations of the system, c) educating users about the risks of AI-related biases, such as decision-automation bias or automation-distrust bias, and d) encouraging users to view the benefits and risks of deploying these systems in terms of their role in helping humans to come to judgements, rather than replacing that judgement.","title":"Explainability"},{"location":"assurance/safe-d-principles/#data-quality-integrity-protection-and-privacy","text":"","title":"Data (Quality, Integrity, Protection and Privacy)"},{"location":"assurance/safe-d-principles/#data-quality","text":"<p>Definition</p> <p>Data quality captures the static properties of data, such as whether they are (a) relevant to and representative of the domain and use context, (b) balanced and complete in terms of how well the dataset represents the underlying data generating process, and \u00a9 up-to-date and accurate as required by the project.</p>     Core Attribute Description     Source Integrity and Measurement Accuracy Effective bias mitigation begins at the very commencement of data extraction and collection processes. Both the sources and instruments of measurement may introduce discriminatory factors into a dataset. When incorporated as inputs in the training data, biased prior human decisions and judgments\u2014such as prejudiced scoring, ranking, interview-data or evaluation\u2014will become the \u2018ground truth\u2019 of the model and replicate the bias in the outputs of the system in order to secure discriminatory non-harm, as well as ensuring that the data sample has optimal source integrity. This involves securing or confirming that the data gathering processes involved suitable, reliable, and impartial sources of measurement and sound methods of collection.   Timeliness and Recency If datasets include outdated data then changes in the underlying data distribution may adversely affect the generalisability of the trained model. Provided these distributional drifts reflect changing social relationship or group dynamics, this loss of accuracy with regard to the actual characteristics of the underlying population may introduce bias into an AI system. In preventing discriminatory outcomes, timeliness and recency of all elements of the data that constitute the datasets must be scrutinised.   Relevance, Appropriateness, and Domain Knowledge The understanding and utilisation of the most appropriate sources and types of data are crucial for building a robust and unbiased AI system. Solid domain knowledge of the underlying population distribution and of the predictive or classificatory goal of the project is instrumental for choosing optimally relevant measurement inputs that contribute to the reasonable determination of the defined solution. Domain experts should collaborate closely with the technical team to assist in the determination of the optimally appropriate categories and sources of measurement.   Adequacy of Quantity and Quality This property involves assessing whether the data available is comprehensive enough to address the problem set at hand, as determined by the use case, domain, function, and purpose of the system. Adequate quantity and quality should address sample size, representativeness, and availability of features relevant to problem.   Balance and Representativeness A balanced and representative dataset is one in which the distribution of features that are included, and the number of samples within each class is similar to the underlying distribution that exists in the overall population.","title":"Data Quality"},{"location":"assurance/safe-d-principles/#data-integrity","text":"<p>Definition</p> <p>'Data Integrity' refers to more dynamic properties of data stewardship, such as how a dataset evolves over the course of a project lifecycle. In this manner, data integrity requires (a) contemporaneous and attributable records from the start of a project (e.g., process logs; research statements), (b) ensuring consistent and verifiable means of data analysis or processing during development, and \u00a9 taking steps to establish findable, accessible, interoperable, and reusable records towards the end of a project\u2019s lifecycle.</p>     Core Attribute Description     Attributable Data should clearly demonstrate who observed and recorded it, when it was observed and recorded, and who it is about.   Consistent, Legible and Accurate Data should be easy to understand, recorded permanently and original entities should be preserved. Data should be free from errors and conform with the protocol. Consistency includes ensuring data is chronological (e.g., has a date and time stamp that is in the expected sequence).   Complete All recorded data requires an audit trail to show nothing has been deleted or lost.   Contemporaneous Data should be recorded as it was observed, and at the time it was executed.   Responsible Data Management Responsible data management ensures that the team has been trained on how to manage data responsibly and securely, identifying possible risks and threats to the system and assigning roles and responsibilities for how to deal with these risks if they were to occur. Policies on data storage and public dissemination of results should be discussed within the team and with stakeholders, as well as being clearly documented.   Data Traceability and Auditability Any changes or revisions to the dataset (e.g., additions, augmentations, normalisation) that occur after the original collection should be clearly traceable and well-documented to support any auditing.","title":"Data Integrity"},{"location":"assurance/safe-d-principles/#data-protection-and-privacy","text":"<p>Definition</p> <p>\u2018Data protection and privacy\u2019 reflect ongoing developments and priorities as set out in relevant legislation and regulation of data practices as they pertain to fundamental rights and freedoms, democracy, and the rule of law. For example, the right for data subjects to have inaccurate personal data rectified or erased.</p>     Core Attribute Description     Consent (or legitimate basis) for processing There must be demonstrable grounds that data processing can be carried out on the basis of the free, specific, informed and unambiguous consent of the data subject or of some other legitimate basis laid down by law. The data subject must be informed of risks that could arise in the absence of appropriate safeguards. Such consent must represent the free expression of an intentional choice, given either by a statement (which can be written, including by electronic means, or oral) or by a clear affirmative action and which clearly indicates in this specific context the acceptance of the proposed processing of personal data. Mere silence, inactivity or pre-validated forms or boxes should not, therefore, constitute consent. No undue influence or pressure (which can be of an economic or other nature) whether direct or indirect, may be exercised on the data subject and consent should not be regarded as freely given where the data subject has no genuine or free choice or is unable to refuse or withdraw consent without prejudice. The data subject has the right to withdraw the consent he or she gave at any time (which is to be distinguished from the separate right to object to processing).   Data Security Each Party shall provide that the controller, and, where applicable the processor, takes appropriate security measures against risks such as accidental or unauthorised access to, destruction, loss, use, modification or disclosure of personal data. Each Party shall provide that the controller notifies, without delay, at least the competent supervisory authority within the meaning of Article 15 of this Convention, of those data breaches which may seriously interfere with the rights and fundamental freedoms of data subjects.   Data Minimisation Personal data being processed is adequate (sufficient to properly fulfil the stated purpose), relevant (has a rational link to that purpose), and limited to what is necessary  do not hold more data than needed for that purpose).   Transparency The transparency of AI systems can refer to several features, both of their inner workings and behaviours, as well as the systems and processes that support them. An AI system is transparent when it is possible to determine how it was designed, developed, and deployed. This can include, among other things, a record of the data that were used to train the system, or the parameters of the model that transforms the input (e.g., an image) into an output (e.g, a description of the objects in the image). However, it can also refer to wider processes, such as whether there are legal barriers that prevent individuals from accessing information that may be necessary to understand fully how the system functions (e.g., intellectual property restrictions).   Proportionality delivering a just outcome in ways that are proportionate to the cost, complexity, and resources available. In a similar vein, the term \u2018proportionality\u2019 can also be used as an evaluative notion, such as in the case of a data protection principle that states only personal data that are necessary and adequate for the purposes of the task are collected.   Purpose Limitation The purposes for data processing must be outlined and documented from the beginning and made available to all individuals through privacy information. Personal data must adhere to the original purpose unless it is compatible with the original purpose, additional consent is received, or there is an obligation or function set out in law.   Lawfulness, fairness, and transparency These three principles necessitate \u2018lawful basis\u2019 for the collection and use of personal data. Personal data must be used in a fair manner that is not unduly detrimental, unexpected, or misleading. Any processes in which data is used should not be in breach of any other laws, and teams must be clear, open, and honest with individuals about how their personal data is being used.   Respect for the rights of data subjects Respect for the rights of data subjects requires putting in place adequate mechanisms or undertaking necessary actions so as to ensure that the rights of data subjects as defined under Convention 108+ and GDPR are upheld. Where necessary, this includes the responsible handling of sensitive data.    <p>Collectively, the above principles and corresponding list of core attributes may appear overly demanding. However, they are not intended to represent a checklist, for which all projects must provide documentation to demonstrate that they have addressed each requirement. Instead, the principles and attributes should be treated as deliberative prompts\u2014a \"reflect-list\" rather than a checklist. If, during initial scoping activities or project planning, and in conjunction with stakeholders, it is determined that certain attributes are irrelevant to the project for justifiable reasons, then no activities may be necessary. An additional principle of proportionality, therefore, should be adopted as a meta-principle of sorts, helping direct project governance decisions.</p> <p>But, if a goal and corresponding set of attributes are deemed important, then the next step is to consider what system or project properties need to be established, and what evidence must be gathered and documented to demonstrate that the top-level goal (and corresponding attributes) have been obtained.</p> <p>The next page offers guidance on this, showing how the principles and activities are integrated into the model of the project lifecycle already introduced.</p>","title":"Data Protection and Privacy"},{"location":"case-studies/case-study-1/","text":"","title":"Recognition: Autonomous Vehicles"},{"location":"case-studies/case-study-1/#tags-case-study","text":"","title":"tags: <code>case study</code>"},{"location":"case-studies/case-study-1/#1-summary","text":"<p>In this case study, we define and discuss the topic of autonomous vehicles (AVs) with respect to algorithmic techniques that engage in Recognition. </p> <p>Specifically, we follow a firm through the process of developing and deploying a new AV to market. We examine the definition of Recognition as an AI category, the key classification data for AVs, the applicable regulatory framework for bringing AVs to market (as well as any gaps in the current regulation). We also look at the relevant ethical considerations involved, and the potential parties in the value which chain might be affected.</p>","title":"1. Summary"},{"location":"case-studies/case-study-1/#2-case-study-details","text":"","title":"2. Case Study Details"},{"location":"case-studies/case-study-1/#case-study-description","text":"<p>A company is in the process of deploying to market a new self-driving car. The AV enables a person to sit behind the steering wheel without controlling either the steering or acceleration/braking of the vehicle, but expects the person to pay attention and to take control of the vehicle if an unexpected event occurs. In this case, a \"transition demand\" notification will ask the user-in-charge to take active control of the vehicle.</p> <p>As will be discussed in greater length below, some of the main regulatory challenges of getting a viable product to market occur where the level of driving automation is such that the vehicle is self-driving but where a human is still expected to remain in charge when an problem arises which the AV cannot handle on its own (a transition demand).</p>","title":"Case Study Description"},{"location":"case-studies/case-study-1/#category-definition-what-is-recognition","text":"<p> </p> <p>Recognition is the identification of different objects or persons as members of categories based on their specific properties or characteristics.</p> <p>In the context of AI, recognition has a wide range of applications (eg identity verification, autonomous navigation), depending on the type of data available (such as text, images, videos, audio, etc). Convolutional neural networks are a common type of AI technique used for recognition (eg facial recognition).</p> <p>The output of a recognition task is usually a label or tag which categorises data. For example, in image recognition the label might be \u201cThis is a tree\u201d; in audio recognition, \u201cThis is the sound of a barking dog\u201d; and in handwriting recognition, \u201cThis is a letter C\u201d. </p> <p>A model or system can be defined as undertaking a recognition task if it takes either structured or unstructured data as input (D) and generates an output variable (O) that classifies the data input as belonging to some set or category.</p>","title":"Category Definition: What is Recognition?"},{"location":"case-studies/case-study-1/#background","text":"<p>Key terms include:</p> <ul> <li>\u201cUser-in-charge\u201d: The person in the driver\u2019s seat, who is expected to regain control of the vehicle in the event of a transition demand.</li> <li>\"Transition Demand\": An event where the AV would alert the user-in-charge to take charge of the vehicle.</li> <li>\"No-user-in-charge\": Situations where certain features--such as valet parking--might gain regulatory approval such that no humans are expected to be in charge of the vehicle.</li> <li>\"Authorised Self-Driving Entity\": The entity that seeks regulatory approval for automated features and retains legal responsibility for them.</li> </ul>","title":"Background"},{"location":"case-studies/case-study-1/#key-information","text":"<p> </p> <ul> <li>SIC Classification<ul> <li>Section: C</li> <li>Class: 29.100 (Manufacture of motor vehiles)</li> </ul> </li> <li>Stakeholders and Other Affected Individuals:<ul> <li>Users of AVs</li> <li>Other drivers on the road, pedestrians, cyclists, etc.</li> <li>AV manufacturers</li> <li>Value chain parties (see below) -Data Types:</li> <li>Sensory Data <ul> <li>External Environment (e.g. LiDAR, Radar, Camera)</li> <li>Internal Environment (e.g. vehicle speed, fuel levels)</li> </ul> </li> <li>Geospatial Data (e.g. roads, location)</li> </ul> </li> <li>Possible Algorithmic Techniques Employed: <ul> <li>Partially Observable Markov Decision Processes (Decision-Making)</li> <li>Deep Reinforcement Learning (Object Detection)</li> </ul> </li> </ul>","title":"Key Information"},{"location":"case-studies/case-study-1/#3-regulatory-considerations","text":"","title":"3. Regulatory Considerations"},{"location":"case-studies/case-study-1/#specific-issues","text":"<ul> <li>Ultimate liability: The central question when regulating AVs is who should be responsible when incidents occur. Different jurisdictions have taken different approaches. For instance, France and Germany have made the user-in-charge liable in certain cases, whereas the UK could move toward a 'no-liability for the occupants of the vehicle' model in the case of certain pre-approved features. The proposed UK model is for the Authorised Self-Driving Entity (ASDE) to secure an additional authorisation as a \"no-user-in-charge operator\". Here, the ASDE would have legal liability rather than the individuals using the AV. This distinction is captured in the image below:</li> </ul> <p></p> <p>When is the ASDE actually at fault? Should liability lie with the ASDE or with the occupants of the vehicle?</p>","title":"Specific Issues"},{"location":"case-studies/case-study-1/#existing-and-proposed-uk-regulation-or-legislation","text":"<p>The UK is likely to make this distinction on a feature by feature basis. Special legal accountability rules will apply depending on whether a certain feature requires a user-in-charge: 1.  If there is a user-in-charge: This person will be responsible for insurance and checking loads, and of taking over control over the car in response to a \u201ctransition demand\u201d situation where the vehicle encounters an issue. This person will have immunity from driving functions such as dangerous or careless driving, speed checks, or running a red light. 2.  In some cases, the vehicle can be approved on behalf of an Authorised Self-Driving Entity (or ASDE) as capable of having \u201cno user-in-charge\u201d (NUIC). NUIC features are those where all occupants of the vehicle are passengers and no one is a user-in-charge. Legal responsibility for safe driving would lie with a licensed NUIC operator. 4.  The Automated and Electric Vehicles Act 2018 will govern civil liability questions. Injured parties would not need to prove fault, but would instead be directly compensated by insurers.</p> <p>Other regulatory considerations:</p> <ul> <li>\"Passivity Problem\": People find it more difficult to monitor a task passively than to be actively engaged in it. A person might be sitting behind the wheel, but once the vehicle is self-driving, that person will not engage their full attention to driving and will have limited ability to respond appropriately to events.</li> <li>\"Vulnerable users\": AVs should not put some groups at greater risk. For instance, vulnerable road users may include pedestrians, cyclists, motorcyclists and horse riders. Moreover, risk should not be increased for groups falling into protected characteristics, such as disability or age. For instance, testing may have been insufficient to determine an AV\u2019s safety impact on wheelchair users. The ASDE should conduct an equality impact assessment when approving AVs.</li> <li>Equality: As a public authority, the Secretary of State is subject to the public sector equality duty. The expansion of AI-technologies into public roads means that public authorities must take the interests of protected groups into consideration when updating licencing and road safety laws.</li> <li>\"Transition Demand\": To what extent should a user-in-charge  be expected to respond to events? Should a user-in-charge be liable in the absence of a transition demand from the AV to the person in the driving seat? For instance, German legislation requires users to respond to \u201cobvious circumstances,\u201d and French legislation requires users to respond to emergency services vehicles. The Law Commission of England and Wales and the Scottish Law Commission take the view that these exceptions require too much if the user is not normally expected to pay attention to the driving task. Instead, they state that an  automated driving system (ADS) feature must be safe even in the absence of human intervention; in case of events such as floods or emergency vehicles, the AV should issue a transition demand to the user-in-charge or come to a safe stop.</li> <li>\"Duty of Candour\": Organisations responsible for AVs would have a duty of candour regarding any incidents, making it a criminal offence to (1) fail to provide information to the regulator or (2) provide information to the regulator that is false or misleading. </li> <li>Privacy laws and data protection: AVs will record and store data, including local data, to permit analysis of vehicle collisions.  ASDEs will be responsible for ensuring that AV data complies with data protection laws. </li> </ul>","title":"Existing and Proposed UK Regulation or Legislation"},{"location":"case-studies/case-study-1/#gaps-in-current-framework","text":"<ul> <li>Regulatory gap: As currently conceived, EDRs will not record details of individual vehicles. They cannot be used to investigate individual incidents. AVs will need to store more data than are captured by EDRs alone. </li> <li>Cybersecurity issues: One safety issue is the potential for interference with AVs, ranging from computer hacking to interfering with a vehicle\u2019s sensors. Hacking is a criminal offence under the Computer Misuse Act 1990. Existing laws would likely cover most instances of intentional interference with AVs. However, section 25 of the Road Traffic Act 1988 must be amended it to make it an offence to tamper with any significant part of an AV.</li> <li>Marketing: Marketing must also be monitored, as consumers should not be given the misleading impression that they have no responsibilities in an AV. Furthermore, marketing must not mislead consumers into thinking vehicles have self-driving capabilities greater than they are in reality. On the flip side, marketing should not be used as a way to avoid liability (e.g. Tesla using Beta software and placing liability on the shoulders of the user).</li> </ul>","title":"Gaps in Current Framework"},{"location":"case-studies/case-study-1/#socioeconomic-value-chains","text":"<p>Legal liability for AVs ultimately lies with the ASDE. The ASDE must conduct an equality impact assessment and submit it alongside a safety case for the AV in question. As the Law Commission of England and Wales &amp; the Scottish Law Commission have noted, there might be an opportunity to meet the needs of an underserved population: </p>  <p>One of the main advantages of AVs is their potential to make life better for those with disabilities. They provide hope to people currently unable to drive that they may be able to own a car. They also have the potential to offer new forms of on-demand services.</p>  <p>Other parties affected include:</p> <ul> <li>Insurers</li> <li>Certifying companies</li> <li>Marketing companies</li> <li>User education companies</li> <li>The issuance of \u201cinterim passenger permits\u201d  would allow passenger service providers to have conditional operational approval of AVs pending further evidence on how AVs interact with passenger safety and accessibility</li> <li>Other impacted industries -- providers of vehicles such as bicycles, motorcycles, scooters, etc.</li> </ul>","title":"Socioeconomic Value Chains"},{"location":"case-studies/case-study-1/#harms-and-benefits","text":"<p>Based on initial empirical results, AVs are expected to be a lot safer than human drivers. The question is how much safer they must be relative to humans in order for us to accept their rate of incidents.</p> <p>AVs operate with incomplete knowledge of their environment. This happens because their sensors are, typically, designed to measure some specific properties of the environment, but have limitations in terms of their range and accuracy. These can include possible occlusions (e.g. inability to perceive one object behind another) or latency issues (i.e. the time it takes for the sensor and system to process information in a dynamic environment.</p> <p>As such, AVs are inherently probabilistic in how they operate. That is, the accuracy with which they recognise objects can be expressed in probabilistic terms (e.g. a confidence rating). Furthemore, they can possess priors about their environment, which are constantly updated as new information is collected. For instance, an AV could have a high prior belief of encountering a car on the road, and a low prior belief of encountering a horse and cart. The ability for AVs to take decisions based on the available information is, therefore, a complex combination of current and prior information.</p> <p>Beyond the regulatory issues discussed above, other ethical issues involving potential harms include:</p> <ul> <li>Practical decision-making: How should the AV decide whom to protect? </li> <li>When is an ADS good enough? An ADS is the combination of software and hardware that perform the driving task. A vehicle may have several ADS features.</li> <li>Explainability of AV systems: What level of explainability is morally expected when incidents occur? Who are the moral agents responsible for collisions and deaths?</li> </ul>","title":"Harms and Benefits"},{"location":"case-studies/case-study-4/","text":"","title":"Personalisation: Price Discrimination"},{"location":"case-studies/case-study-4/#tags-case-study","text":"","title":"tags: <code>case study</code>"},{"location":"case-studies/case-study-4/#1-summary","text":"","title":"1. Summary"},{"location":"case-studies/case-study-4/#2-case-study-details","text":"","title":"2. Case Study Details"},{"location":"case-studies/case-study-4/#case-study-description","text":"<p>An online travel booking platform decides to implement a flight aggregator that shows customers different prices for the same flight depending on information inferred from their browsing habits and digital fingerprints (e.g. location from IP address, household income based on data extracted from broswer cookies). </p> <p>The platform groups users into different buckets, which it uses to estimate their 'willingness to pay'\u2014an inferred category. Customers that have visited sites associated with luxury brands, or who are accessing the platform from more affluent postcodes, are charged a premium compared to other users.</p> <p>Customers are, typically, not aware or informed that this is happening, but the platform does have a permissive privacy policy in place that can be accessed by users. Furthermore, because the prices of the goods change frequently, consumers do not have a straightforward way of comparing prices among different customers.1 The online retailer is engaging in price discrimination, in particular what is known as targeted pricing.</p>","title":"Case Study Description"},{"location":"case-studies/case-study-4/#category-definition-what-is-personalisation","text":"<p> </p> <p>In a general sense, personalisation is the targeting or adapting of some behaviour or outcome to a specific person. In the context of AI, personalisation occurs when an algorithm learns about an individual's profile in order to tailor the services or products offered to that person. </p> <p>A range of AI techniques can be employed in personalisation, including learning algorithms that create and refine a model of a specific user over time based on the user's behaviour and interaction with the algorithmic system.</p> <p>Recommendation systems are a well-known application of personalisation. For instance, an algorithm can provide recommendations on what content to watch or listen to based on a consumer's previous browsing or purchase patterns.</p>","title":"Category Definition: What is Personalisation?"},{"location":"case-studies/case-study-4/#background","text":"<p>Key terms include:</p>","title":"Background"},{"location":"case-studies/case-study-4/#key-information","text":"<p> </p> <ul> <li>SIC Classification<ul> <li>Section: G</li> <li>Class: All codes which involve retail sales. (list?)</li> </ul> </li> <li>Stakeholders and Other Affected Individuals:<ul> <li>Online shoppers </li> <li>Retailers</li> <li>System Developer</li> </ul> </li> <li>Data Types:<ul> <li>Digital Fingerptints (e.g. IP Address)</li> <li>Browser Cookies</li> </ul> </li> <li>Possible Algorithmic Techniques Employed:<ul> <li>Unsupervised Learning (e.g. clustering) to segment consumers during market research</li> <li>Multi-class classification to predict which group a specific user is most closely aligned to</li> <li>Reinforcement learning to identify and optimise personalised 'willingness to pay'</li> </ul> </li> </ul>","title":"Key Information"},{"location":"case-studies/case-study-4/#3-regulatory-considerations","text":"","title":"3. Regulatory Considerations"},{"location":"case-studies/case-study-4/#specific-issues","text":"","title":"Specific Issues"},{"location":"case-studies/case-study-4/#existing-and-proposed-uk-regulation-or-legislation","text":"","title":"Existing and Proposed UK Regulation or Legislation"},{"location":"case-studies/case-study-4/#gaps-in-current-framework","text":"","title":"Gaps in Current Framework"},{"location":"case-studies/case-study-4/#socioeconomic-value-chains","text":"","title":"Socioeconomic Value Chains"},{"location":"case-studies/case-study-4/#harms-and-benefits","text":"<ol> <li> <p>Travel sites have been known to do a version of this. They charge travelers from some countries a premium when they book flights or hotel rooms from specific countries (Rose &amp; Rahman, 2015). Additionally, the prices for flights and hotel rooms fluctuate with relative high frequency, which makes it hard for individual connsumers to keep track of them and compare.\u00a0\u21a9</p> </li> </ol>","title":"Harms and Benefits"},{"location":"case-studies/cs-introduction/","text":"<p></p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus bibendum congue consectetur. Vivamus varius sollicitudin tincidunt. Fusce rutrum egestas orci quis lacinia. Fusce id leo eget elit gravida fermentum ac nec neque. Quisque ac tincidunt ligula. Ut eget lorem eros. Nunc id vestibulum arcu. Suspendisse maximus quam quis pellentesque ullamcorper. Integer volutpat lacus in eros placerat, ut elementum purus venenatis. Phasellus mattis tincidunt enim, ac commodo sem auctor convallis. Quisque at felis nulla. Fusce hendrerit vehicula erat, vitae laoreet odio. Aenean sodales nunc et enim suscipit egestas. Pellentesque elementum, eros eget posuere efficitur, quam ex congue tortor, at egestas risus odio sit amet turpis. Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p> <p>Phasellus ante arcu, tempor sed lacus sit amet, cursus interdum urna. Proin vel ultricies nibh. Vestibulum ex mauris, ornare eu porttitor ut, interdum non mauris. Vivamus mattis ut odio ac fermentum. Integer non congue mauris. Nunc porttitor posuere sapien in efficitur. Fusce ornare fermentum turpis id gravida. Nam at felis a dui tristique efficitur nec consectetur justo.</p> <p>Ut cursus purus vel urna euismod, non egestas nulla consectetur. Mauris scelerisque turpis massa, et sodales dui aliquam et. In eget dolor convallis, sollicitudin neque posuere, tristique velit. Nunc id volutpat velit. Suspendisse consectetur libero ut lacus venenatis facilisis. Nullam vitae nunc sollicitudin erat tempor fringilla et ut ligula. Suspendisse at sem libero. Donec id orci id lorem ultricies accumsan a in elit. Donec a dolor enim. Sed non luctus purus. Aliquam convallis blandit augue in pretium. Maecenas id quam lacus. Quisque turpis lorem, consequat vitae dignissim nec, porttitor et nisi. In condimentum ante vitae nisl pellentesque gravida. Nam at magna vitae metus tristique aliquam.</p>","title":"Case Studies"},{"location":"case-studies/cs-introduction/#overview","text":"<ul> <li> <p> Case Study 1</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Case Study 2</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Case Study 3</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Case Study 4</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Case Study 5</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Case Study 6</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Case Study 7</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> </ul>","title":"Overview"},{"location":"case-studies/how-to/","text":"<p></p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus bibendum congue consectetur. Vivamus varius sollicitudin tincidunt. Fusce rutrum egestas orci quis lacinia. Fusce id leo eget elit gravida fermentum ac nec neque. Quisque ac tincidunt ligula. Ut eget lorem eros. Nunc id vestibulum arcu. Suspendisse maximus quam quis pellentesque ullamcorper. Integer volutpat lacus in eros placerat, ut elementum purus venenatis. Phasellus mattis tincidunt enim, ac commodo sem auctor convallis. Quisque at felis nulla. Fusce hendrerit vehicula erat, vitae laoreet odio. Aenean sodales nunc et enim suscipit egestas. Pellentesque elementum, eros eget posuere efficitur, quam ex congue tortor, at egestas risus odio sit amet turpis. Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p> <p>Phasellus ante arcu, tempor sed lacus sit amet, cursus interdum urna. Proin vel ultricies nibh. Vestibulum ex mauris, ornare eu porttitor ut, interdum non mauris. Vivamus mattis ut odio ac fermentum. Integer non congue mauris. Nunc porttitor posuere sapien in efficitur. Fusce ornare fermentum turpis id gravida. Nam at felis a dui tristique efficitur nec consectetur justo.</p> <p>Ut cursus purus vel urna euismod, non egestas nulla consectetur. Mauris scelerisque turpis massa, et sodales dui aliquam et. In eget dolor convallis, sollicitudin neque posuere, tristique velit. Nunc id volutpat velit. Suspendisse consectetur libero ut lacus venenatis facilisis. Nullam vitae nunc sollicitudin erat tempor fringilla et ut ligula. Suspendisse at sem libero. Donec id orci id lorem ultricies accumsan a in elit. Donec a dolor enim. Sed non luctus purus. Aliquam convallis blandit augue in pretium. Maecenas id quam lacus. Quisque turpis lorem, consequat vitae dignissim nec, porttitor et nisi. In condimentum ante vitae nisl pellentesque gravida. Nam at magna vitae metus tristique aliquam.</p>","title":"How to Use the Case Studies"},{"location":"case-studies/how-to/#overview","text":"<ul> <li> <p> Recognition</p>  <p>Autonomous Vehicles</p> <p> Go to chapter</p> </li> <li> <p> Case Study 2</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Case Study 3</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Personalisation</p>  <p>Price Discrimination</p> <p> Go to chapter</p> </li> <li> <p> Case Study 5</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Case Study 6</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> <li> <p> Case Study 7</p>  <p>Case study summary...</p> <p> Go to chapter</p> </li> </ul>","title":"Overview"},{"location":"lifecycle/pl-introduction/","text":"<p></p> <p>This section introduces our model of the AI project lifecycle, and provides detailed descriptions of the constitutive stages.</p> <p>The model we propose is focused on research and innovation because this allows us to embed ethical, social, and legal principles within the actual practices and processes that are (in part) responsible for the myriad social harms and benefits that have been identified by those working in data and AI ethics (broadly construed). Nevertheless, the conception of the project lifecycle that we propose is a socially situated one. This prevents our model from being myopically focused on research and innovation practices, to the neglect of other perspectives, such that vital sociopolitical contextual factors are overlooked or ignored.</p> <p>The following cards can be used to navigate to the relevant pages of this section.</p>","title":"The AI Project Lifecycle"},{"location":"lifecycle/pl-introduction/#section-overview","text":"<ul> <li> <p> What is the Project Lifecycle?</p>  <p>In this section we introduce the model at a general level, and explain its value and purpose for regulators and policy-makers.</p> <p> Go to section</p> </li> <li> <p> Project Design</p>  <p>This section focuses on the first stage of the project lifecycle (project design) and its constitutive activities.</p> <p> Go to section</p> </li> <li> <p> Model Development</p>  <p>This section focuses on the first stage of the project lifecycle (model development) and its constitutive activities.</p> <p> Go to section</p> </li> <li> <p> System Deployment</p>  <p>This section focuses on the first stage of the project lifecycle (system deployment) and its constitutive activities.</p> <p> Go to section</p> </li> </ul>","title":"Section Overview"},{"location":"lifecycle/project-design/","text":"<p>The (project) design stage comprises the following activities:</p> <ul> <li>Project planning</li> <li>Problem Formulation</li> <li>Data Extraction and Procurement</li> <li>Data Analysis</li> <li>Preprocessing and Feature Engineering</li> </ul>","title":"(Project) Design"},{"location":"lifecycle/project-lifecycle/","text":"<p>There are many ways of carving up the lifecycle for a data science or AI project. For instance, (Sweenor, 2020) breaks it into four stages: Build, Manage, Deploy &amp; Integrate, Monitor.1 Ashmore et al. (2019) also identify four stages, which have a more specific focus on data science: data management, model learning, model verification, and model deployment.</p> <p>The multiplicity of approaches is likely a product of the evolution of diverse methods in data mining/analytics, the significant impact of ML on research and innovation, and the specific practices and considerations inherent to each of the various domains where ML techniques are applied. While there are many benefits of existing frameworks, they have not been designed with a responsible and ethical approach to governance in mind.</p> <p>The following figure, therefore, presents a model of a typical lifecycle for a project involving data science or the production of an ML/AI system. We have designed this model to support the ethical governance and responsible regulation of AI/ML, while remaining faithful to the technical processes. However, it is important to note that the model is a heuristic device, and, therefore, is not intended to be perfectly represent all ML or AI projects.</p>  <p>Info</p> <p>You can hover over the individual activity bubbles to see a description of the corresponding activity.</p>   <p>                               Preliminary activities designed to help scope out the aims, objectives and processes involved with the project, including potential risks and benefits. Project Planning        The formulation of a clear statement about the over-arching problem the system or project addresses (e.g., a research statement or system specification) and a lower-level description of the computational procedure that instantiates it. ProblemFormulation        The design of an experimental method or decisions about data gathering and collection, based on the planning and problem formulation from the previous steps. Data Extraction&amp; Procurement        Stages of exploratory and confirmatory data analysis designed to help researchers or developers identify relevant associations between input variables and target variables. Data Analysis        A process of cleaning, normalising, and refactoring data into the features that will be used in model training and testing, as well as the features that may be used in the final system. Preprocessing&amp; FeatureEngineering        The selection of a particular algorithm (or multiple algorithms) for training the model. Model Selection &amp; Training        An algorithmic model that that adapts its behaviour over time or context may require updating or deprovisioning (i.e. removing from the production environment). Model Updating&amp; Deprovisioning        Ongoing monitoring and feedback from the system, either automated or probed, to ensure that issues such as model drift have not affected performance or resulted in harms to individuals or groups. System Use&amp; Monitoring        Training for those individuals or groups who are either required to operate a data-driven system (perhaps in a safety critical context) or who are likely to use the system (e.g. consumers). User Training         The process of putting a model into production, and implementing the operational system, which enables and structures interaction with the model, within the respective environment (e.g., a recommender system that converts a user's existing movie ratings into recommendations for future watches).                             A process of documenting both the formal and non-formal properties of both the model and the processes by which it was developed (e.g., source of data, algorithms used, evaluation metrics). ModelReporting        Testing the model against a variety of metrics, which may include those that assess how accurate a model is for different sub-groups of a population. This is important where issues of fairness or equality may arise. Model Testing &amp; Validation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </p> <p>To begin, the inner circle breaks the project lifecycle into three processes:</p> <ol> <li>(Project) Design</li> <li>(Model) Development</li> <li>(System) Deployment</li> </ol> <p>These terms are intended to be maximally inclusive. For example, the design stage encompasses any project task or decision-making process that scaffolds or sets constraints on later project stages (i.e. design system constraints).</p> <p>Each of the stages shades into its neighbours, as there is no clear boundary that differentiates certain project design activities (e.g. data extraction and exploratory analysis) from model design activities (e.g. preprocessing and feature engineering, model selection). As such, the design stage overlaps with the development stage, but the latter extends to include the actual process of training, testing, and validating a ML model. Similarly, the process of productionalising a model within a system can be thought of as both a development and deployment activity. And, so, the deployment stage overlaps with the \u2018development\u2019 stage, and also overlaps with the \u2018design\u2019 stage because the deployment of a system should be thought of as an ongoing process (e.g. where new data are used to continuously train the ML model, or, the decision to de-provision a model may require the planning and design of a new model if the older (legacy) system becomes outdated).</p> <p>Despite the unidirectional nature of the arrows, we also acknowledge that ML/AI research and innovation is frequently an iterative process. Therefore, the singular direction is only present at a macroscopic level of abstraction (i.e., the overall direction of progress for a project), and allows for some inevitable back and forth between the stages at the microscopic level.</p> <p>The three higher-level stages can be thought of as a useful heuristic for approaching the project lifecycle. However, each higher-level stage subsumes a wide variety of tasks and activities that are likely to be carried out by different individuals, teams, and organisations, depending on their specific roles and responsibilities (e.g. procurement of data). Therefore, it is important to break each of the three higher-level stages into their (typical) constituent parts, which are likely to vary to some extent between specific projects or within particular organisations.</p> <p>The following pages explore each of these stages in detail.</p>   <ol> <li> <p>These four stages are influenced by an \u2018ML OPs\u2019 perspective. The term \u2018MLOps\u2019 refers to the application of DevOps practices to ML pipelines. The term is often used in an inclusive manner to incorporate traditional statistical or data science practices that support the ML lifecycle, but are not themselves constitutive of machine learning (e.g. exploratory data analysis), as well as deployment practices that are important within business and operational contexts (e.g. monitoring key performance indicators).\u00a0\u21a9</p> </li> </ol>","title":"What is the AI Project Lifecycle?"},{"location":"lifecycle/property-claims/","text":"<p>On this page we offer some illustrative examples that demonstrate how the SAFE-D principles and attributes can be used to support risk management and responsible research and innovation, by scaffolding the task of identifying specific actions or decisions that can be undertaken during the project lifecycle.</p>    Principle &amp; Attribute Action or Decision Project Lifecycle Stage     Sustainability (Robustness) The model used in our system has been internally and externally validated. The external validation has been carried out across several varied environments to ensure robustness of the system. Model Training, Testing and Validation   Accountability (Accessibility) All identified stakeholders were consulted prior to the development of our system to help critically evaluate our project plans and ensure they were intelligible. Project Planning and Problem Formulation   Fairness (Equality) Persons affected by use of the system have avenues of recourse, ability to contest system outputs and demand human intervention. System Use &amp; Monitoring   Explainability (Responsible Model Selection) Features were hand-selected in conjunction with domain experts to optimise for both interpretability and predictive power. Preprocessing &amp; Feature Engineering and Model Selection   Data Quality (Timeliness &amp; Recency) Only data that were collected within the previous 3 months were used to ensure the training data were up-to-date. Data Extraction or Procurement","title":"From Principles to Practice"},{"location":"regulation/re-introduction/","text":"<p></p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus bibendum congue consectetur. Vivamus varius sollicitudin tincidunt. Fusce rutrum egestas orci quis lacinia. Fusce id leo eget elit gravida fermentum ac nec neque. Quisque ac tincidunt ligula. Ut eget lorem eros. Nunc id vestibulum arcu. Suspendisse maximus quam quis pellentesque ullamcorper. Integer volutpat lacus in eros placerat, ut elementum purus venenatis. Phasellus mattis tincidunt enim, ac commodo sem auctor convallis. Quisque at felis nulla. Fusce hendrerit vehicula erat, vitae laoreet odio. Aenean sodales nunc et enim suscipit egestas. Pellentesque elementum, eros eget posuere efficitur, quam ex congue tortor, at egestas risus odio sit amet turpis. Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p> <p>Phasellus ante arcu, tempor sed lacus sit amet, cursus interdum urna. Proin vel ultricies nibh. Vestibulum ex mauris, ornare eu porttitor ut, interdum non mauris. Vivamus mattis ut odio ac fermentum. Integer non congue mauris. Nunc porttitor posuere sapien in efficitur. Fusce ornare fermentum turpis id gravida. Nam at felis a dui tristique efficitur nec consectetur justo.</p> <p>Ut cursus purus vel urna euismod, non egestas nulla consectetur. Mauris scelerisque turpis massa, et sodales dui aliquam et. In eget dolor convallis, sollicitudin neque posuere, tristique velit. Nunc id volutpat velit. Suspendisse consectetur libero ut lacus venenatis facilisis. Nullam vitae nunc sollicitudin erat tempor fringilla et ut ligula. Suspendisse at sem libero. Donec id orci id lorem ultricies accumsan a in elit. Donec a dolor enim. Sed non luctus purus. Aliquam convallis blandit augue in pretium. Maecenas id quam lacus. Quisque turpis lorem, consequat vitae dignissim nec, porttitor et nisi. In condimentum ante vitae nisl pellentesque gravida. Nam at magna vitae metus tristique aliquam.</p>","title":"Regulating the AI Lifecycle"},{"location":"regulation/re-introduction/#overview","text":"<ul> <li> <p> Section 1</p>  <p>Section summary...</p> <p> Go to section</p> </li> <li> <p> Section 2</p>  <p>Section summary...</p> <p> Go to section</p> </li> <li> <p> Section 3</p>  <p>Section summary...</p> <p> Go to section</p> </li> </ul>","title":"Overview"}]}